{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA9nvfDB1bl4",
        "outputId": "c5562a7d-d2d5-492d-b42e-7cbf2d4096e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikeras[tensorflow]\n",
            "  Downloading scikeras-0.10.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras[tensorflow]) (23.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras[tensorflow]) (1.2.2)\n",
            "Requirement already satisfied: tensorflow>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from scikeras[tensorflow]) (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.11.0->scikeras[tensorflow]) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.11.0->scikeras[tensorflow]) (0.1.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (3.2.2)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.10.0\n",
            "/bin/bash: conda: command not found\n"
          ]
        }
      ],
      "source": [
        "!pip install keras\n",
        "!pip install scikeras[tensorflow]\n",
        "!conda install pytorch torchvision torchaudio cudatoolkit 10.2 -c pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx4s27NN2j8A",
        "outputId": "fc6dd0c0-d55b-4194-80b5-e72c124fdf90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from flair.models import TextClassifier\n",
        "# from flair.data import Sentence\n",
        "\n",
        "# Importing of various classification tools that were tested\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "# Used classification tools\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# To save Sentiment Analysis\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuPd6y0D4rzM"
      },
      "outputs": [],
      "source": [
        "with gzip.open('/content/drive/MyDrive/12/CDs_and_Vinyl/train/training_data_with_sentiments.json', 'rb') as f:\n",
        "    training_data = pickle.load(f)\n",
        "scores = pd.read_json('/content/drive/MyDrive/12/CDs_and_Vinyl/train/product_training.json')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data.to_pickle('/content/drive/MyDrive/12/CDs_and_Vinyl/train/training_data_uncompressed.pkl')\n"
      ],
      "metadata": {
        "id": "smoqml8iSWRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3DpDWlr4B-E"
      },
      "outputs": [],
      "source": [
        "def create_feature_vector(training_data,scores):\n",
        "    # Preprocessing & Feature Generation\n",
        "    training_data[\"format\"] = training_data[\"style\"].apply(lambda x: x[\"Format:\"] if pd.notna(x) else None)\n",
        "    # New Features\n",
        "    training_data[\"review_length\"] = training_data[\"reviewText\"].apply(lambda text: len(text) if text != None else 0)\n",
        "    training_data[\"vote\"] = training_data[\"vote\"].apply(lambda vote: 0 if pd.isna(vote) else int(vote.replace(',', '')) if isinstance(vote, str) else int(vote))\n",
        "    # Reviewer Ratio Generation\n",
        "    reviewers = training_data[['reviewerID', 'verified']].copy()\n",
        "    reviewers['verification_ratio'] = reviewers['verified'].map(int)\n",
        "    reviewers = reviewers.drop('verified', axis=1)\n",
        "    reviewers = reviewers.groupby(['reviewerID']).mean()\n",
        "    training_data = pd.merge(training_data, reviewers, on='reviewerID', how='left')\n",
        "\n",
        "    summary_stats = pd.DataFrame()\n",
        "    summary_stats['raw'] = training_data[['asin', 'summary_sentiment']].groupby('asin')['summary_sentiment'].apply(list)\n",
        "    summary_stats['summary_sentiment_avg'] = summary_stats.raw.apply(lambda x: np.mean(x))\n",
        "    summary_stats['summary_sentiment_std'] = summary_stats.raw.apply(lambda x: np.std(x))\n",
        "\n",
        "    summary_stats['number_of_reviews'] = summary_stats.raw.apply(lambda list: pd.Series(list).count())\n",
        "\n",
        "    text_stats = pd.DataFrame()\n",
        "    text_stats['raw'] = training_data[['asin', 'text_sentiment']].groupby('asin')['text_sentiment'].apply(list)\n",
        "    text_stats['text_sentiment_avg'] = text_stats.raw.apply(lambda x: np.mean(x))\n",
        "    text_stats['text_sentiment_std'] = text_stats.raw.apply(lambda x: np.std(x))\n",
        "\n",
        "    vote_weighted_stats = pd.DataFrame(training_data['asin'])\n",
        "    vote_weighted_stats['vote_weighted_summary_sentiment'] = training_data['summary_sentiment'] * training_data['vote']\n",
        "    vote_weighted_stats['vote_weighted_text_sentiment'] = training_data['text_sentiment'] * training_data['vote']\n",
        "    vote_weighted_stats = vote_weighted_stats.groupby('asin').mean()\n",
        "\n",
        "    verified_weighted_stats = pd.DataFrame(training_data['asin'])\n",
        "    verified_weighted_stats['verified_text_sentiment'] = training_data['verified'] * training_data['text_sentiment']\n",
        "    verified_weighted_stats['verified_summary_sentiment'] = training_data['verified'] * training_data['summary_sentiment']\n",
        "\n",
        "    verified_weighted_stats['verified_vote_weighted_text_sentiment'] = training_data['verified'] * training_data['text_sentiment'] * training_data['vote']\n",
        "    verified_weighted_stats['verified_vote_weighted_summary_sentiment'] = training_data['verified'] * training_data['summary_sentiment'] * training_data['vote']\n",
        "    verified_weighted_stats = verified_weighted_stats.groupby('asin').mean()\n",
        "\n",
        "    time_weighted_stats = pd.DataFrame(training_data['asin'])\n",
        "    training_data['normedReviewTime'] = (training_data['unixReviewTime'] - training_data['unixReviewTime'].mean()) / training_data['unixReviewTime'].std()\n",
        "    time_weighted_stats['avgReviewTime'] = training_data['normedReviewTime']\n",
        "\n",
        "    product_format = training_data[['asin', 'format']]\n",
        "    product_format.drop_duplicates(subset=['asin'])\n",
        "    format_list = product_format.format.unique()\n",
        "    formats = {}\n",
        "    for i in range(len(format_list)):\n",
        "        formats[format_list[i]] = i\n",
        "    product_format.format = product_format.format.map(formats)\n",
        "    product_format = product_format.drop_duplicates(subset='asin')\n",
        "\n",
        "    aggregatedProductFeatures = pd.merge(summary_stats.drop('raw', axis=1), text_stats.drop('raw', axis=1), on='asin')\n",
        "    aggregatedProductFeatures = aggregatedProductFeatures.merge(vote_weighted_stats, on='asin').merge(verified_weighted_stats, on='asin').merge(time_weighted_stats, on='asin')\n",
        "\n",
        "    aggregatedProductFeatures = aggregatedProductFeatures.merge(product_format, on='asin')\n",
        "    verified_review_ratio = training_data[['asin', 'verified']].groupby('asin').mean()\n",
        "    aggregatedProductFeatures = aggregatedProductFeatures.merge(verified_review_ratio, on='asin')\n",
        "    aggregatedProductFeatures['text_sentiment_std'] = aggregatedProductFeatures['text_sentiment_std'].apply(lambda std: 0 if pd.isna(std) else std)\n",
        "    aggregatedProductFeatures['summary_sentiment_std'] = aggregatedProductFeatures['summary_sentiment_std'].apply(lambda std: 0 if pd.isna(std) else std)\n",
        "    aggregatedProductFeatures = aggregatedProductFeatures.merge(scores, on='asin')\n",
        "    return aggregatedProductFeatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMopBdGzuaJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b51601f4-19ea-49dc-e5b0-8f10c58e5f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-05eb63a4521f>:49: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  product_format.format = product_format.format.map(formats)\n"
          ]
        }
      ],
      "source": [
        "aggregatedProductFeatures = create_feature_vector(training_data,scores)\n",
        "X = aggregatedProductFeatures.drop(['asin', 'awesomeness'], axis=1)\n",
        "y = aggregatedProductFeatures['awesomeness']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIUfBifZzHHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86275880-6e05-42dc-d5a6-696b013512aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-05eb63a4521f>:49: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  product_format.format = product_format.format.map(formats)\n"
          ]
        }
      ],
      "source": [
        "with gzip.open('/content/drive/MyDrive/12/CDs_and_Vinyl/train/data_with_SA.json', 'rb') as f:\n",
        "    old_data = pickle.load(f)\n",
        "old_features = create_feature_vector(old_data,scores)\n",
        "old_X = old_features.drop(['asin', 'awesomeness'], axis=1)\n",
        "old_y = old_features['awesomeness']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgZMoGcFbqEf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gzip.open('/content/drive/MyDrive/12/CDs_and_Vinyl/test3/test3_with_sentiments.json', 'rb') as f:\n",
        "    test_3_data = pickle.load(f)\n",
        "product_test_3 = pd.read_json('/content/drive/MyDrive/12/CDs_and_Vinyl/test3/product_test.json')\n",
        "\n",
        "features = create_feature_vector(test_3_data , product_test_3)\n",
        "test_3_X = features.drop(['asin'], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFTFwVnwwIiy",
        "outputId": "8a956860-1ee4-4045-9583-ff96469d4812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-05eb63a4521f>:49: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  product_format.format = product_format.format.map(formats)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_3_data.to_pickle('/content/drive/MyDrive/12/CDs_and_Vinyl/train/test_3_data.pkl')"
      ],
      "metadata": {
        "id": "etssk8phaW7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFDctpF6NpI4",
        "outputId": "162a467e-bbdf-4b18-949a-019f976b7303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  2.3min remaining:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.7min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flair's: 0.7479841291392126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  2.3min remaining:  1.6min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaludi's: 0.6692727613234261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.7min finished\n"
          ]
        }
      ],
      "source": [
        "# COMPARING FLAIR AND KALUDI- SLIDE 3 & 7\n",
        "bg = BaggingClassifier(DecisionTreeClassifier(max_depth = 9, max_features = 0.7), max_samples = 0.25, max_features = 1.0, n_estimators = 100)\n",
        "\n",
        "f1_scores = cross_val_score(bg, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "print(f\"Flair's: {np.mean(f1_scores)}\")\n",
        "\n",
        "f1_scores = cross_val_score(bg, old_X, old_y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "print(f\"Kaludi's: {np.mean(f1_scores)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J-HZhxzjzNF",
        "outputId": "3558a2c1-c7de-4ffe-ea48-0fb3a1eec798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "Best parameters found:\n",
            " {'n_neighbors': 200, 'weights': 'distance'}\n",
            "0.618 (+/-0.032) for {'n_neighbors': 5, 'weights': 'uniform'}\n",
            "0.617 (+/-0.032) for {'n_neighbors': 5, 'weights': 'distance'}\n",
            "0.642 (+/-0.031) for {'n_neighbors': 100, 'weights': 'uniform'}\n",
            "0.643 (+/-0.030) for {'n_neighbors': 100, 'weights': 'distance'}\n",
            "0.651 (+/-0.033) for {'n_neighbors': 200, 'weights': 'uniform'}\n",
            "0.653 (+/-0.031) for {'n_neighbors': 200, 'weights': 'distance'}\n"
          ]
        }
      ],
      "source": [
        "# GRIDSEARCH OF KNN- SLIDE 4\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "parameter_space = {\n",
        "    'n_neighbors': [5, 100, 200],\n",
        "    'weights': ['uniform', 'distance']\n",
        "}\n",
        "knn = KNeighborsClassifier()\n",
        "clf = GridSearchCV(knn, parameter_space, cv=3, scoring=make_scorer(f1_score), n_jobs=-1, verbose=3)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Best parameter set\n",
        "print('Best parameters found:\\n', clf.best_params_)\n",
        "\n",
        "means = clf.cv_results_['mean_test_score']\n",
        "stds = clf.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRIDSEARCH OF DECISION TREE- SLIDE 4\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "\n",
        "param_distributions = {\n",
        "    'max_depth': [5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5, 10],\n",
        "    'max_features': [.1,.5,.7,1],\n",
        "}\n",
        "dt = DecisionTreeClassifier()\n",
        "f1_scorer = make_scorer(f1_score)\n",
        "random_search = RandomizedSearchCV(estimator=dt, \n",
        "                                   param_distributions=param_distributions,\n",
        "                                   cv=3,\n",
        "                                   n_iter=7, \n",
        "                                   scoring=f1_scorer,\n",
        "                                   random_state=42)\n",
        "random_search.fit(X, y)\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Best F1 score: {best_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oml4Tf8UAkSG",
        "outputId": "595bc13b-7985-4ce8-d659-460c7af1ddc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 0.7, 'max_depth': 10}\n",
            "Best F1 score: 0.7224957226555073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nw2Ph4ClwYrt",
        "outputId": "47034c8a-67b9-4ce5-c167-dbab4110fcd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters found:\n",
            " {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.601 (+/-0.075) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.762 (+/-0.013) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.656 (+/-0.071) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.760 (+/-0.027) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.601 (+/-0.048) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.809 (+/-0.038) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.693 (+/-0.005) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.825 (+/-0.024) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.615 (+/-0.066) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.869 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.698 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.875 (+/-0.015) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.565 (+/-0.024) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.727 (+/-0.014) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.688 (+/-0.005) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.736 (+/-0.004) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.575 (+/-0.035) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.731 (+/-0.005) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.689 (+/-0.021) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.722 (+/-0.028) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.581 (+/-0.103) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.736 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.691 (+/-0.013) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.728 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.596 (+/-0.051) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.715 (+/-0.009) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.678 (+/-0.024) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.715 (+/-0.001) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.587 (+/-0.024) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.719 (+/-0.004) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.657 (+/-0.067) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.716 (+/-0.008) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.623 (+/-0.039) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.708 (+/-0.017) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.673 (+/-0.020) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.719 (+/-0.006) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.534 (+/-0.071) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.765 (+/-0.021) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.657 (+/-0.157) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.752 (+/-0.028) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.590 (+/-0.077) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.771 (+/-0.011) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.715 (+/-0.014) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.774 (+/-0.026) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.599 (+/-0.153) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.785 (+/-0.026) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.712 (+/-0.042) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.801 (+/-0.008) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.652 (+/-0.128) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.743 (+/-0.005) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.656 (+/-0.159) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.740 (+/-0.009) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.574 (+/-0.054) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.750 (+/-0.004) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.552 (+/-0.019) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.744 (+/-0.011) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.649 (+/-0.071) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.751 (+/-0.007) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.658 (+/-0.045) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.749 (+/-0.002) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.682 (+/-0.021) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.720 (+/-0.006) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.654 (+/-0.146) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.717 (+/-0.009) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.558 (+/-0.009) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.719 (+/-0.006) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.606 (+/-0.149) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.715 (+/-0.004) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.554 (+/-0.010) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.717 (+/-0.018) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.634 (+/-0.123) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.723 (+/-0.001) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.737 (+/-0.016) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.740 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.640 (+/-0.032) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.744 (+/-0.005) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.678 (+/-0.033) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.738 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.655 (+/-0.027) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.743 (+/-0.001) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.689 (+/-0.011) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.745 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.639 (+/-0.041) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.720 (+/-0.011) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.677 (+/-0.023) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.720 (+/-0.009) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.631 (+/-0.023) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.722 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.685 (+/-0.013) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.720 (+/-0.009) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.645 (+/-0.002) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.718 (+/-0.010) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.692 (+/-0.002) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.724 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (32, 32, 32), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (64, 64, 64), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.544 (+/-0.000) for {'activation': 'logistic', 'alpha': 1, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# GRIDSEARCH WITH NEURAL NETS- SLIDE 4\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(32,32,32), (64,64,64), (128,128,128)],\n",
        "    'activation': ['tanh', 'relu','logistic'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.01, 0.1,1],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}\n",
        "\n",
        "mlp = MLPClassifier(max_iter=100)\n",
        "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = .2)\n",
        "clf.fit(X_train, y_train)\n",
        "print('Best parameters found:\\n', clf.best_params_)\n",
        "\n",
        "means = clf.cv_results_['mean_test_score']\n",
        "stds = clf.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7VEWIW4eu4s",
        "outputId": "ec6132f5-4cea-4eb2-9847-735102d8b66f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.73456299 0.73405119 0.73854831 0.73911818 0.77828765 0.73807748\n",
            " 0.72556275 0.74015515 0.73662935 0.73439033]\n",
            "0.7399383372123085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.1min finished\n"
          ]
        }
      ],
      "source": [
        "# BASIC LOGISTIC REGRESSION WITH FLAIR- SLIDE 5\n",
        "clf = LogisticRegression(C = 4.28)\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "\n",
        "print(f1_scores)\n",
        "print(np.mean(f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbMlbQpItSnM",
        "outputId": "826b4c3e-c689-4eea-8b26-f0e4d4b5f9b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  8.7min remaining:  5.8min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.72940984 0.73545922 0.73317762 0.76208795 0.79089447 0.76001864\n",
            " 0.73992483 0.72680963 0.7430821  0.75059279]\n",
            "0.7471457079498535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 14.0min finished\n"
          ]
        }
      ],
      "source": [
        "# BASIC RANDOMFOREST WITH FLAIR- SLIDE 5\n",
        "clf = RandomForestClassifier(max_depth = 9, max_features = 0.7, n_estimators = 185, max_samples = 0.8)\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "\n",
        "print(f1_scores)\n",
        "print(np.mean(f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f57hUHtCKYw"
      },
      "outputs": [],
      "source": [
        "# BASIC SVM WITH FLAIR- SLIDE 5\n",
        "clf = SVC(kernel = 'rbf', C = 0.1, gamma = 1)\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "\n",
        "print(f1_scores)\n",
        "print(np.mean(f1_scores))\n",
        "####################################################\n",
        "# *Compute time took too long*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe99Fo0JF9_c",
        "outputId": "f7ab5186-fa29-4d13-feba-7e38b0cf1ff1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   56.9s remaining:   37.9s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6567748187596336\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.4min finished\n"
          ]
        }
      ],
      "source": [
        "# BASIC SVM WITH FLAIR- SLIDE 5\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "clf = KNeighborsClassifier(weights = 'distance', n_neighbors=200)\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "print(np.mean(f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4NutXhP-Uvn",
        "outputId": "329b5307-1c50-402f-967e-09d60c1eff27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed: 28.0min remaining: 18.7min\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean cross-validation score:  0.7038565712408165\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 38.6min finished\n"
          ]
        }
      ],
      "source": [
        "# BASIC NEURAL NETS WITH FLAIR- SLIDE 5\n",
        "best_params = {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
        "mlp_best = MLPClassifier(**best_params, max_iter=100)\n",
        "f1_scores = cross_val_score(mlp_best, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "\n",
        "print(\"Mean cross-validation score: \", f1_scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BASIC DECISION TREE WITH FLAIR- SLIDE 5\n",
        "clf = DecisionTreeClassifier(max_depth = 10, max_features = .7, min_samples_leaf = 5, min_samples_split = 10)\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "print(np.mean(f1_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgVt909sJhue",
        "outputId": "56c385bc-c1f7-467c-f382-75454325fb19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    6.3s remaining:    4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.721651415685223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.8s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oklavnXnlGwV",
        "outputId": "e05dc19f-7f51-4b0c-ab4f-98d5484260d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed: 16.7min remaining: 11.2min\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.73454121 0.73452029 0.73535395 0.7389692  0.7788288  0.75049516\n",
            " 0.73746492 0.73667733 0.72786726 0.72994232]\n",
            "0.7404660441287559\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 27.2min finished\n"
          ]
        }
      ],
      "source": [
        "# LOGISTIC REGRESSION WITH FLAIR + BAGGING- SLIDE 6\n",
        "clf = BaggingClassifier(LogisticRegression(C = 4.28), max_samples = 0.25, max_features = 1.0, n_estimators = 100)\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "\n",
        "print(f1_scores)\n",
        "print(np.mean(f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep6ROt5swJSV",
        "outputId": "b53b00c1-784b-4be1-8900-32e440d35e9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed: 158.1min remaining: 105.4min\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.80309096 0.71913954 0.69558421 0.7174121  0.71685454 0.70859308\n",
            " 0.69712593 0.70072274 0.68984812 0.68916298]\n",
            "0.7137534203787574\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 256.0min finished\n"
          ]
        }
      ],
      "source": [
        "# RANDOMFOREST WITH FLAIR + BAGGING- SLIDE 6\n",
        "clf = BaggingClassifier(RandomForestClassifier(max_depth = 9, max_features = 0.7, n_estimators = 185, max_samples = 0.8), max_samples = 0.25, max_features = 1.0, n_estimators = 100)\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "\n",
        "print(f1_scores)\n",
        "print(np.mean(f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Er3El1aCcY5",
        "outputId": "0cb0bdd0-69f1-49c2-8695-65fc2bdb86d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.65339982 0.67143514 0.66473    0.67150934 0.72308234 0.64835635\n",
            " 0.66162112 0.65729028 0.66185767 0.6505248 ]\n",
            "0.6663806850891638\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 63.3min finished\n"
          ]
        }
      ],
      "source": [
        "# KNN WITH FLAIR + BAGGING- SLIDE 6\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "clf = BaggingClassifier(KNeighborsClassifier(n_neighbors=200),  max_samples = 0.25, max_features = 1.0, n_estimators = 100)\n",
        "\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "\n",
        "print(f1_scores)\n",
        "print(np.mean(f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzbbX9icI6CS",
        "outputId": "0abe84f3-7dca-41a3-d2b8-94dd4b285e90"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ]
        }
      ],
      "source": [
        "# NEURAL NETS WITH FLAIR + BAGGING- SLIDE 6\n",
        "best_params = {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
        "clf = BaggingClassifier(MLPClassifier(**best_params, max_iter=100),   max_samples = 0.25, max_features = 1.0, n_estimators = 100)\n",
        "# Perform cross-validation with 10 folds\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "\n",
        "print(\"Mean cross-validation score: \", f1_scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DECISION TREE WITH FLAIR + BAGGING- SLIDE 6\n",
        "clf = BaggingClassifier(DecisionTreeClassifier(max_depth = 10, max_features = .7, min_samples_leaf = 5, min_samples_split = 10), max_samples = 0.25, max_features = 1.0, n_estimators = 100)\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "print(np.mean(f1_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgCzUfIQCTy7",
        "outputId": "a1c8396c-a85b-492a-be59-c74fdf27e211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  2.5min remaining:  1.7min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7529036132822367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  4.0min finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnHivJXFmY6t",
        "outputId": "e774eb63-203d-4cb3-c0d2-b64ffec2f7ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  2.2min remaining:  1.4min\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.72595252 0.73128639 0.73229809 0.73380815 0.77445968 0.73740273\n",
            " 0.72254964 0.73630778 0.7270296  0.71018757]\n",
            "0.7331282166853124\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.2min finished\n"
          ]
        }
      ],
      "source": [
        "# LOGISTIC REGRESSION WITH FLAIR + BOOSTING- SLIDE 7\n",
        "clf = AdaBoostClassifier(base_estimator = LogisticRegression(C = 4.28),n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "\n",
        "print(f1_scores)\n",
        "print(np.mean(f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_ydrvkdxfwd"
      },
      "outputs": [],
      "source": [
        "# RANDOM FOREST WITH FLAIR + BOOSTING- SLIDE 7\n",
        "clf = AdaBoostClassifier(RandomForestClassifier(max_depth = 9, max_features = 0.7, n_estimators = 185, max_samples = 0.8),n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "\n",
        "print(f1_scores)\n",
        "print(np.mean(f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEnnfaFQUAEx"
      },
      "outputs": [],
      "source": [
        "# NEURAL NETS WITH FLAIR + BOOSTING- SLIDE 7\n",
        "best_params = {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (128, 128, 128), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
        "clf = AdaBoostClassifier(MLPClassifier(**best_params, max_iter=100),n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "\n",
        "print(\"Mean cross-validation score: \", f1_scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DECISION TREE WITH FLAIR + BOOSTING- SLIDE 7\n",
        "clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 10, max_features = .7, min_samples_leaf = 5, min_samples_split = 10),n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
        "f1_scores = cross_val_score(clf, X, y, cv=10, scoring=\"f1\", n_jobs=-1, verbose = 1)\n",
        "print(np.mean(f1_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A62KYaVSKTrh",
        "outputId": "2502b0b2-9f4b-49b4-be12-867827f09a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  4.4min remaining:  2.9min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.684876835462896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  6.9min finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the Best Performing Model\n",
        "best_clf = BaggingClassifier(DecisionTreeClassifier(max_depth = 10, max_features = .7, min_samples_leaf = 5, min_samples_split = 10), max_samples = 0.25, max_features = 1.0, n_estimators = 100)\n",
        "\n",
        "best_clf.fit(X,y)\n",
        "\n",
        "# Assume you have a model called 'model'\n",
        "with open('/content/drive/MyDrive/12/CDs_and_Vinyl/train/best_clf.json', 'wb') as f:\n",
        "    pickle.dump(best_clf, f)"
      ],
      "metadata": {
        "id": "AW5Aij-wsV3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/12/CDs_and_Vinyl/train/best_clf.json', 'rb') as f:\n",
        "    best_clf = pickle.load(f)"
      ],
      "metadata": {
        "id": "513kxo6lvsLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features['review_predictions'] = best_clf.predict(test_3_X)"
      ],
      "metadata": {
        "id": "88eNMT7DEvTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean review prediction per product and round it\n",
        "predictions = features.groupby('asin')['review_predictions'].mean().round()\n",
        "\n",
        "# Convert Series to DataFrame\n",
        "predictions_df = predictions.reset_index()\n",
        "\n",
        "# Rename columns\n",
        "predictions_df.columns = ['asin', 'awesomeness']\n",
        "\n",
        "# Convert 'awesomeness' to integer\n",
        "predictions_df['awesomeness'] = predictions_df['awesomeness'].astype(int)\n",
        "\n",
        "# Save to JSON\n",
        "predictions_df.to_json('/content/drive/MyDrive/12/CDs_and_Vinyl/train/predictions.json', orient='records')"
      ],
      "metadata": {
        "id": "O7RTlDJyQB8N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}